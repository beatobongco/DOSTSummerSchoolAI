{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Just ignore prefaced with \"--- TEST ---\", they're there for debugging. Thanks! - BRB \n",
    "\n",
    "Questions\n",
    "\n",
    "* CountVectorizer.fit has optional y parameter - X\n",
    "* order of nb,class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# DOST AI Summer School 2017\n",
    "# Multinomial Naive Bayes Spam Classifier\n",
    "\n",
    "Prepared by Jerelyn Co (ADMU) and Hadrian Paulo Lim (ADMU) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Practicals: Spam Filtering with Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "2. Representing text as numerical data\n",
    "3. Reading a text-based dataset into pandas\n",
    "4. Vectorizing our dataset\n",
    "5. Building and evaluating a model\n",
    "6. Comparing models\n",
    "7. Examining a model for further insight\n",
    "9. Tuning the vectorizer (challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Part 1: Representing text as numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# example text for model training\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "\n",
    "We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "# using the variable name vect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the 'vocabulary' of the training data (occurs in-place)\n",
    "# by calling vect.fit() on the simple_train array\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "# using the transform() method of vect on the simple_train array\n",
    "# Assign the result to a variable simple_train_dtm\n",
    "simple_train_dtm = vect.transform(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the document-term matrix\n",
    "type(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# examine the sparse matrix contents\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n",
    "\n",
    "> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"please don't call me\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "**Summary:**\n",
    "\n",
    "- `vect.fit(train)` **learns the vocabulary** of the training data\n",
    "- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n",
    "- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data (and **ignores tokens** it hasn't seen before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Part 3: Reading a text-based dataset into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# read file into pandas using a relative path\n",
    "# assign the DataFrame object to a variable called spam_ham.\n",
    "# Use the extra parameters: header=0, names=['label', 'location','message']\n",
    "# for the read function\n",
    "path = 'data/spam_ham.csv'\n",
    "spam_ham = None\n",
    "# Drop entries with null values using dropna(inplace=True) on your DataFrame\n",
    "spam_ham = pd.read_csv(path, header=0, names=['label', 'location', 'message'])\n",
    "spam_ham = spam_ham.drop('location', axis=1)\n",
    "spam_ham.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30974, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape\n",
    "# You should get (30974, 2)\n",
    "spam_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>LUXURY WATCHES - BUY YOUR OWN ROLEX FOR ONLY $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Academic Qualifications available from prestig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Greetings all. This is to verify your subscrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>try chauncey may conferred the luscious not co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>It's quiet. Too quiet. Well, how about a straw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>It's working here. I have departed almost tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>The OIL sector is going crazy. This is our wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spam</td>\n",
       "      <td>Little magic. Perfect weekends.http://othxu.rz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>Greetings all. This is a mass acknowledgement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi, L C P A X V V e I r m a A I v A o b n L A ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0  spam  LUXURY WATCHES - BUY YOUR OWN ROLEX FOR ONLY $...\n",
       "1  spam  Academic Qualifications available from prestig...\n",
       "2   ham  Greetings all. This is to verify your subscrip...\n",
       "3  spam  try chauncey may conferred the luscious not co...\n",
       "4   ham  It's quiet. Too quiet. Well, how about a straw...\n",
       "5   ham  It's working here. I have departed almost tota...\n",
       "6  spam  The OIL sector is going crazy. This is our wee...\n",
       "7  spam  Little magic. Perfect weekends.http://othxu.rz...\n",
       "8   ham  Greetings all. This is a mass acknowledgement ...\n",
       "9  spam  Hi, L C P A X V V e I r m a A I v A o b n L A ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 10 rows\n",
    "spam_ham[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam    19280\n",
       "ham     11694\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "spam_ham.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# convert the labels to a numerical variable\n",
    "# where ham is reassigned to 0, and spam is 1.\n",
    "# The converted labels should be under the label_num column.\n",
    "spam_ham['label_num'] = spam_ham.label.map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>LUXURY WATCHES - BUY YOUR OWN ROLEX FOR ONLY $...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Academic Qualifications available from prestig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Greetings all. This is to verify your subscrip...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>try chauncey may conferred the luscious not co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>It's quiet. Too quiet. Well, how about a straw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>It's working here. I have departed almost tota...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>The OIL sector is going crazy. This is our wee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spam</td>\n",
       "      <td>Little magic. Perfect weekends.http://othxu.rz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>Greetings all. This is a mass acknowledgement ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Hi, L C P A X V V e I r m a A I v A o b n L A ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0  spam  LUXURY WATCHES - BUY YOUR OWN ROLEX FOR ONLY $...          1\n",
       "1  spam  Academic Qualifications available from prestig...          1\n",
       "2   ham  Greetings all. This is to verify your subscrip...          0\n",
       "3  spam  try chauncey may conferred the luscious not co...          1\n",
       "4   ham  It's quiet. Too quiet. Well, how about a straw...          0\n",
       "5   ham  It's working here. I have departed almost tota...          0\n",
       "6  spam  The OIL sector is going crazy. This is our wee...          1\n",
       "7  spam  Little magic. Perfect weekends.http://othxu.rz...          1\n",
       "8   ham  Greetings all. This is a mass acknowledgement ...          0\n",
       "9  spam  Hi, L C P A X V V e I r m a A I v A o b n L A ...          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the conversion worked\n",
    "spam_ham.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30974,)\n",
      "(30974,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = spam_ham.message\n",
    "y = spam_ham.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "# Use the ff. variables: X_train, X_test, y_train, y_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18642    This weeks pick. New News Just out. Should Spa...\n",
      "4023     New Release This one is from the Oil Sector. W...\n",
      "18093    CWTD Has Wild 10 days as Stock climbs from $1....\n",
      "14245    >>looking to match full mail names against pot...\n",
      "26957    Find local gir1s eager to meeet you now!100000...\n",
      "Name: message, dtype: object\n",
      "18642    1\n",
      "4023     1\n",
      "18093    1\n",
      "14245    0\n",
      "26957    1\n",
      "Name: label_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- TEST ---\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Part 4: Vectorizing our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer and assign it to vect again.\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "vect.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '000000',\n",
       " '00000000',\n",
       " '000000000',\n",
       " '00000000000000',\n",
       " '000000000000000000000000000000049999999999999e9',\n",
       " '0000000000000000000000000000000500000000000000e9',\n",
       " '0000000000000016666l',\n",
       " '0000000000000017d',\n",
       " '00000000000000e',\n",
       " '000000000000received',\n",
       " '000000000001received',\n",
       " '0000000000status',\n",
       " '0000000001d0',\n",
       " '0000000001l0',\n",
       " '0000000010000000004l0',\n",
       " '0000000016',\n",
       " '000000001d0',\n",
       " '00000000message',\n",
       " '00000000x',\n",
       " '00000001',\n",
       " '00000001content',\n",
       " '00000001irdecode',\n",
       " '00000004',\n",
       " '00000010',\n",
       " '00000010pwm',\n",
       " '00000011',\n",
       " '0000001196',\n",
       " '00000049',\n",
       " '0000005',\n",
       " '0000006hz',\n",
       " '000000eb',\n",
       " '000001',\n",
       " '00000100shaftencoder',\n",
       " '00000111',\n",
       " '000001bdaaa0',\n",
       " '000001bdb744',\n",
       " '000001bdc5a5',\n",
       " '000001bdd411',\n",
       " '000001bdd98c',\n",
       " '000001bdda70',\n",
       " '000001bde0a0',\n",
       " '000001bed6b7',\n",
       " '000001c642d0',\n",
       " '000001c64310',\n",
       " '000001c64562',\n",
       " '000001c64585',\n",
       " '000001c64615',\n",
       " '000001c6465f',\n",
       " '000001c6468e',\n",
       " '000001c676b8',\n",
       " '0000040b',\n",
       " '0000040c',\n",
       " '000005',\n",
       " '00000dd0',\n",
       " '00001',\n",
       " '000010',\n",
       " '00001000',\n",
       " '00001004',\n",
       " '00001008',\n",
       " '0000100c',\n",
       " '00001010',\n",
       " '00001014',\n",
       " '00001018',\n",
       " '0000101c',\n",
       " '00001023',\n",
       " '00001026',\n",
       " '00001028',\n",
       " '0000102c',\n",
       " '00001030',\n",
       " '00001034',\n",
       " '00001038',\n",
       " '0000103e',\n",
       " '00001040',\n",
       " '00001044',\n",
       " '00001048',\n",
       " '0000104c',\n",
       " '00001050',\n",
       " '00001054',\n",
       " '0000105a',\n",
       " '0000105ci',\n",
       " '00001808',\n",
       " '0000180cmain',\n",
       " '00002i',\n",
       " '00003001722',\n",
       " '000031',\n",
       " '000046',\n",
       " '000061',\n",
       " '000066',\n",
       " '000076',\n",
       " '000092',\n",
       " '00009800',\n",
       " '00009804',\n",
       " '0000bd',\n",
       " '0000bi',\n",
       " '0000comment',\n",
       " '0000date',\n",
       " '0000delivered',\n",
       " '0000ff',\n",
       " '0000from',\n",
       " '0000importance',\n",
       " '0000in',\n",
       " '0000instituteof0000',\n",
       " '0000message',\n",
       " '0000mime',\n",
       " '0000nline',\n",
       " '0000nology',\n",
       " '0000our',\n",
       " '0000received',\n",
       " '0000reply',\n",
       " '0000to',\n",
       " '0000x',\n",
       " '0001',\n",
       " '000101376290',\n",
       " '000101bdc5a5',\n",
       " '000101bed6b7',\n",
       " '000101bedba4',\n",
       " '000101bef4c2',\n",
       " '00010246',\n",
       " '000107',\n",
       " '00011',\n",
       " '00014',\n",
       " '0001_mayprod',\n",
       " '0001jn',\n",
       " '0001pt',\n",
       " '0001we',\n",
       " '0001x',\n",
       " '0002',\n",
       " '000201bdaabb',\n",
       " '000201bef248',\n",
       " '000201c265a3',\n",
       " '000232026900114623590008952623',\n",
       " '00024355',\n",
       " '000255000',\n",
       " '0002phone',\n",
       " '0003',\n",
       " '000301bde673',\n",
       " '000301c634d3',\n",
       " '000301c6430e',\n",
       " '0003gy',\n",
       " '0003qj',\n",
       " '0003zh',\n",
       " '0004',\n",
       " '000401bdd670',\n",
       " '000401bde673',\n",
       " '000401bee39f',\n",
       " '00048z',\n",
       " '0004ek',\n",
       " '0004ti',\n",
       " '0005',\n",
       " '000501bdd670',\n",
       " '000501bed6db',\n",
       " '000501c62cfa',\n",
       " '000548',\n",
       " '0005859375',\n",
       " '0005iw',\n",
       " '0006',\n",
       " '000601bdd38d',\n",
       " '000601bed721',\n",
       " '000601bef689',\n",
       " '0006eu',\n",
       " '0006ms',\n",
       " '0006pn',\n",
       " '0006sc',\n",
       " '0006z8',\n",
       " '0007',\n",
       " '000701bee330',\n",
       " '0007051519140',\n",
       " '00077u',\n",
       " '0007ka',\n",
       " '0008',\n",
       " '0009',\n",
       " '000901c0e23e',\n",
       " '000a',\n",
       " '000a01bde6a3',\n",
       " '000abpz20',\n",
       " '000antiprotons',\n",
       " '000bachelor',\n",
       " '000barrels',\n",
       " '000br',\n",
       " '000c01c67da1',\n",
       " '000company',\n",
       " '000copies',\n",
       " '000d01c0dff7',\n",
       " '000d93c57554',\n",
       " '000dear',\n",
       " '000doctorate',\n",
       " '000e01bee437',\n",
       " '000e137038fe07f0ea1e00000e1370ad14f0a238060170380382783800fc7f18157f941b',\n",
       " '000equals',\n",
       " '000eur',\n",
       " '000f01bee9ca',\n",
       " '000f01c0fa97',\n",
       " '000hours',\n",
       " '000in',\n",
       " '000industry',\n",
       " '000job',\n",
       " '000km',\n",
       " '000live',\n",
       " '000master',\n",
       " '000miles',\n",
       " '000mk',\n",
       " '000no',\n",
       " '000or',\n",
       " '000over',\n",
       " '000person',\n",
       " '000rpm',\n",
       " '000solar',\n",
       " '000some',\n",
       " '000students',\n",
       " '000summary',\n",
       " '000t',\n",
       " '000uponcompletion',\n",
       " '000yes',\n",
       " '000you',\n",
       " '000контактные',\n",
       " '000アブノーマル1時間',\n",
       " '000元',\n",
       " '000元以內',\n",
       " '000円',\n",
       " '000円のみが必要となりますが',\n",
       " '000円不要',\n",
       " '000性感ノーマル',\n",
       " '001',\n",
       " '0010',\n",
       " '00100001',\n",
       " '001001bedde0',\n",
       " '001001bee9ca',\n",
       " '001001befa5e',\n",
       " '001001bf09ef',\n",
       " '00101380381e0700ea1fff5b13f8ea17e00010c7fca6ea11f8ea120cea1c07381803801210380001c0a214e0a4127012f0a200e013c01280ea4003148038200700ea1006ea0c1cea03f013227ea018',\n",
       " '0011',\n",
       " '001101bdd428',\n",
       " '001111111',\n",
       " '0012',\n",
       " '001201bee9d7',\n",
       " '001201beedd8',\n",
       " '00126',\n",
       " '0013',\n",
       " '0014',\n",
       " '001401bf04c9',\n",
       " '001401bf09f4',\n",
       " '0015',\n",
       " '001532edt',\n",
       " '0016',\n",
       " '001601bdd428',\n",
       " '0017',\n",
       " '001701bdd73e',\n",
       " '001701bed5b2',\n",
       " '001701bef454',\n",
       " '0018',\n",
       " '001801beda0f',\n",
       " '001888',\n",
       " '0019',\n",
       " '001a01bef1c4',\n",
       " '001aa',\n",
       " '001b01bf0a4a',\n",
       " '001e133000231370ea438014e01283ea8700a2380701c0120ea3381c0380a4eb0700a35bea0c3eea03ceea000ea25b1260eaf0381330485aea80c0ea4380003ec7fc141f7b9418',\n",
       " '001e1360002313e0ea4380eb81c01283ea8701a238070380120ea3381c0700a31408eb0e101218121ceb1e20ea0c263807c3c015157b941a',\n",
       " '001eb1ec',\n",
       " '001eboard',\n",
       " '001eeb60e00023ebe0f0384380e1eb81c000831470d887011330a23907038020120ea3391c070040a31580a2ec0100130f380c0b02380613843803e0f81c157b9420',\n",
       " '001f01bdd431',\n",
       " '001f01c0faab',\n",
       " '001f01c275ca',\n",
       " '001fb512f8391e03c03800181418123038200780a200401410a2eb0f001280a200001400131ea45ba45ba45ba4485aa41203b5fc1d2277a123',\n",
       " '001fboard',\n",
       " '001fd',\n",
       " '001http',\n",
       " '001kp',\n",
       " '001mk',\n",
       " '002',\n",
       " '0020',\n",
       " '002001bdd439',\n",
       " '00203228888',\n",
       " '0020afec18ca',\n",
       " '0020j',\n",
       " '0021',\n",
       " '002101bdd459',\n",
       " '00212',\n",
       " '0022',\n",
       " '002201bdd548',\n",
       " '002249',\n",
       " '00228',\n",
       " '0023',\n",
       " '002301bdd548',\n",
       " '002301c23827',\n",
       " '002321',\n",
       " '00233',\n",
       " '0024',\n",
       " '002401bdd548',\n",
       " '002401c66aeb',\n",
       " '0024________________email',\n",
       " '0025',\n",
       " '0026',\n",
       " '0027',\n",
       " '002712est',\n",
       " '0028',\n",
       " '002801c13bab',\n",
       " '0028ii',\n",
       " '0029',\n",
       " '002930_voxinfo',\n",
       " '0029817706609725333l434294481',\n",
       " '002983',\n",
       " '0029porta',\n",
       " '002_dragon004200943940content',\n",
       " '002_dragon008535651103',\n",
       " '002_dragon013699393919',\n",
       " '002_dragon019357534987content',\n",
       " '002_dragon020181421106',\n",
       " '002_dragon032685489904content',\n",
       " '002_dragon034150072259',\n",
       " '002_dragon034633567807content',\n",
       " '002_dragon039925383399content',\n",
       " '002_dragon040476364422',\n",
       " '002_dragon048889247829content',\n",
       " '002_dragon056675156800',\n",
       " '002_dragon058285243905content',\n",
       " '002_dragon060280103893content',\n",
       " '002_dragon073148149514',\n",
       " '002_dragon074457059520',\n",
       " '002_dragon077329414837',\n",
       " '002_dragon080566096389content',\n",
       " '002_dragon092734619076content',\n",
       " '002_dragon100455099132content',\n",
       " '002_dragon108076263577content',\n",
       " '002_dragon108468622097content',\n",
       " '002_dragon120569444328',\n",
       " '002_dragon122521392039content',\n",
       " '002_dragon128631707502content',\n",
       " '002_dragon133773915096content',\n",
       " '002_dragon136224228356content',\n",
       " '002_dragon138839954923',\n",
       " '002_dragon140844353194',\n",
       " '002_dragon142360745850content',\n",
       " '002_dragon143330654218',\n",
       " '002_dragon149449844292',\n",
       " '002_dragon161509917710',\n",
       " '002_dragon162756092795content',\n",
       " '002_dragon168204929497content',\n",
       " '002_dragon169002695074',\n",
       " '002_dragon169085030403content',\n",
       " '002_dragon176086318908',\n",
       " '002_dragon177944757813content',\n",
       " '002_dragon179326334746',\n",
       " '002_dragon183888777798content',\n",
       " '002_dragon183941553104content',\n",
       " '002_dragon185781585974',\n",
       " '002_dragon186514966961_',\n",
       " '002_dragon188257448684content',\n",
       " '002_dragon188333289331content',\n",
       " '002_dragon189316002559content',\n",
       " '002_dragon193530954717content',\n",
       " '002_dragon196072547742',\n",
       " '002_dragon198189501342content',\n",
       " '002_dragon203013389770content',\n",
       " '002_dragon204052897310',\n",
       " '002_dragon205793002835content',\n",
       " '002_dragon206018273441content',\n",
       " '002_dragon207785784570content',\n",
       " '002_dragon211481662238',\n",
       " '002_dragon212983034666content',\n",
       " '002_dragon213924637584content',\n",
       " '002_dragon214166912429',\n",
       " '002_dragon214895540320',\n",
       " '002_dragon215886918037',\n",
       " '002_dragon216410465126content',\n",
       " '002_dragon231888896784content',\n",
       " '002_dragon233134712957content',\n",
       " '002_dragon234879898522content',\n",
       " '002_dragon235764159952content',\n",
       " '002_dragon237516939735',\n",
       " '002_dragon239516577637content',\n",
       " '002_dragon240032945636content',\n",
       " '002_dragon248564804421content',\n",
       " '002_dragon249354945590content',\n",
       " '002_dragon249807569787content',\n",
       " '002_dragon254481303756',\n",
       " '002_dragon256918362398',\n",
       " '002_dragon264396653478',\n",
       " '002_dragon264441716722content',\n",
       " '002_dragon265325440711content',\n",
       " '002_dragon266209308694content',\n",
       " '002_dragon269013230617content',\n",
       " '002_dragon270367826058content',\n",
       " '002_dragon273382864054content',\n",
       " '002_dragon273895548293',\n",
       " '002_dragon281660342386content',\n",
       " '002_dragon285173245356content',\n",
       " '002_dragon289924567963content',\n",
       " '002_dragon291265061908content',\n",
       " '002_dragon292505557095content',\n",
       " '002_dragon293119990835content',\n",
       " '002_dragon295843020376content',\n",
       " '002_dragon302510766348',\n",
       " '002_dragon302921738635',\n",
       " '002_dragon310127751282content',\n",
       " '002_dragon310651914794content',\n",
       " '002_dragon313149702908',\n",
       " '002_dragon317695925892content',\n",
       " '002_dragon318665289543content',\n",
       " '002_dragon319948883600',\n",
       " '002_dragon320946592690content',\n",
       " '002_dragon322915006407',\n",
       " '002_dragon324589698709content',\n",
       " '002_dragon327437312378content',\n",
       " '002_dragon327643673166content',\n",
       " '002_dragon330573254373content',\n",
       " '002_dragon333436883450content',\n",
       " '002_dragon336214030309content',\n",
       " '002_dragon340834821974',\n",
       " '002_dragon341612040241',\n",
       " '002_dragon343333554216',\n",
       " '002_dragon345724565924content',\n",
       " '002_dragon346394276285content',\n",
       " '002_dragon348016147037content',\n",
       " '002_dragon356616615600content',\n",
       " '002_dragon359805902139content',\n",
       " '002_dragon361767389765',\n",
       " '002_dragon366284922033',\n",
       " '002_dragon370830372714',\n",
       " '002_dragon377070002978content',\n",
       " '002_dragon377441852287',\n",
       " '002_dragon384660614980content',\n",
       " '002_dragon389780210192content',\n",
       " '002_dragon390632298472content',\n",
       " '002_dragon391848222368content',\n",
       " '002_dragon399477789492content',\n",
       " '002_dragon402079065150',\n",
       " '002_dragon402740244099content',\n",
       " '002_dragon409823729458content',\n",
       " '002_dragon410509063251content',\n",
       " '002_dragon411110209560content',\n",
       " '002_dragon413436947052content',\n",
       " '002_dragon415805250710content',\n",
       " '002_dragon416123410162content',\n",
       " '002_dragon425323668785content',\n",
       " '002_dragon431429810021content',\n",
       " '002_dragon431676018905content',\n",
       " '002_dragon432852498332content',\n",
       " '002_dragon435496836113',\n",
       " '002_dragon437917652885content',\n",
       " '002_dragon440034119332',\n",
       " '002_dragon441674338630',\n",
       " '002_dragon442592498835',\n",
       " '002_dragon443472164152content',\n",
       " '002_dragon443689150123content',\n",
       " '002_dragon444004361912content',\n",
       " '002_dragon448288168555content',\n",
       " '002_dragon451319953791',\n",
       " '002_dragon451657700235content',\n",
       " '002_dragon456176210945content',\n",
       " '002_dragon468266803279content',\n",
       " '002_dragon470466639645',\n",
       " '002_dragon473323566965',\n",
       " '002_dragon476540816282content',\n",
       " '002_dragon477399743496content',\n",
       " '002_dragon479331544790content',\n",
       " '002_dragon483579780476content',\n",
       " '002_dragon484358097899content',\n",
       " '002_dragon488902243432',\n",
       " '002_dragon491393046235',\n",
       " '002_dragon495877656850content',\n",
       " '002_dragon503725778714_',\n",
       " '002_dragon504217449345',\n",
       " '002_dragon505110412875',\n",
       " '002_dragon505932322972',\n",
       " '002_dragon506805364816content',\n",
       " '002_dragon508712711421content',\n",
       " '002_dragon512122577234',\n",
       " '002_dragon513692340014content',\n",
       " '002_dragon514676478309content',\n",
       " '002_dragon515128794190',\n",
       " '002_dragon516011979524content',\n",
       " '002_dragon516666948361content',\n",
       " '002_dragon518255923908content',\n",
       " '002_dragon519557785805',\n",
       " '002_dragon520683972230content',\n",
       " '002_dragon522040799568',\n",
       " '002_dragon527791339896content',\n",
       " '002_dragon530821623860content',\n",
       " '002_dragon533304058117_',\n",
       " '002_dragon533835082890content',\n",
       " '002_dragon535670459181content',\n",
       " '002_dragon536989065088content',\n",
       " '002_dragon538638107395content',\n",
       " '002_dragon540629808243content',\n",
       " '002_dragon542105054566content',\n",
       " '002_dragon542522505882content',\n",
       " '002_dragon544018013072content',\n",
       " '002_dragon546536074428content',\n",
       " '002_dragon547831489158',\n",
       " '002_dragon554816776882content',\n",
       " '002_dragon557516700063content',\n",
       " '002_dragon558995758314',\n",
       " '002_dragon560194215614content',\n",
       " '002_dragon561824435930content',\n",
       " '002_dragon563227845761',\n",
       " '002_dragon564322207242content',\n",
       " '002_dragon571434638457',\n",
       " '002_dragon574371893608content',\n",
       " '002_dragon575169100404_',\n",
       " '002_dragon575585758776content',\n",
       " '002_dragon577596397905content',\n",
       " '002_dragon583411697069content',\n",
       " '002_dragon584915480615',\n",
       " '002_dragon585001720044content',\n",
       " '002_dragon585387820253',\n",
       " '002_dragon586232798130content',\n",
       " '002_dragon591660557037content',\n",
       " '002_dragon596828381337content',\n",
       " '002_dragon597439856990',\n",
       " '002_dragon599296306728',\n",
       " '002_dragon610136592431content',\n",
       " '002_dragon610953810273content',\n",
       " '002_dragon619468299972content',\n",
       " '002_dragon623245546434',\n",
       " '002_dragon623495405341content',\n",
       " '002_dragon625493329806content',\n",
       " '002_dragon634337155732content',\n",
       " '002_dragon636962270343content',\n",
       " '002_dragon643501113307',\n",
       " '002_dragon645239832728',\n",
       " '002_dragon645469919759content',\n",
       " '002_dragon647819242747content',\n",
       " '002_dragon648837860654content',\n",
       " '002_dragon660060898871content',\n",
       " '002_dragon660123564464content',\n",
       " '002_dragon664173122785content',\n",
       " '002_dragon669679313152content',\n",
       " '002_dragon671339860625content',\n",
       " '002_dragon671413180375',\n",
       " '002_dragon682176656828',\n",
       " '002_dragon682242077477',\n",
       " '002_dragon683701003467',\n",
       " '002_dragon683848968845',\n",
       " '002_dragon684485696525content',\n",
       " '002_dragon685743176115content',\n",
       " '002_dragon700393232030content',\n",
       " '002_dragon702086349026',\n",
       " '002_dragon706880655832content',\n",
       " '002_dragon710680206378',\n",
       " '002_dragon714483479317content',\n",
       " '002_dragon715506809858content',\n",
       " '002_dragon716684339575_',\n",
       " '002_dragon721447944431content',\n",
       " '002_dragon721496209399content',\n",
       " '002_dragon735922421923content',\n",
       " '002_dragon744366455165',\n",
       " '002_dragon745440808393content',\n",
       " '002_dragon745846869871content',\n",
       " '002_dragon749046174059',\n",
       " '002_dragon749135313276content',\n",
       " '002_dragon749616001928content',\n",
       " '002_dragon749775980445',\n",
       " '002_dragon752042517009content',\n",
       " '002_dragon752940174982content',\n",
       " '002_dragon756883035274content',\n",
       " '002_dragon758472607762',\n",
       " '002_dragon760169235045content',\n",
       " '002_dragon762646400558content',\n",
       " '002_dragon763409779758',\n",
       " '002_dragon773027103006',\n",
       " '002_dragon777504927530',\n",
       " '002_dragon779053823085content',\n",
       " '002_dragon779723947235content',\n",
       " '002_dragon785824725207',\n",
       " '002_dragon785883807422content',\n",
       " '002_dragon787036265046',\n",
       " '002_dragon788617422663',\n",
       " '002_dragon789549662168content',\n",
       " '002_dragon790560539053',\n",
       " '002_dragon790568223600',\n",
       " '002_dragon796954387895content',\n",
       " '002_dragon799105077372',\n",
       " '002_dragon803951708982content',\n",
       " '002_dragon805058253897content',\n",
       " '002_dragon806793363072',\n",
       " '002_dragon809654126877_',\n",
       " '002_dragon809864011866content',\n",
       " '002_dragon813633483461content',\n",
       " '002_dragon813978104655',\n",
       " '002_dragon820046601046content',\n",
       " '002_dragon821031089314content',\n",
       " '002_dragon833788956448_',\n",
       " '002_dragon835344230368content',\n",
       " '002_dragon835873675296content',\n",
       " '002_dragon836071345290',\n",
       " '002_dragon837387913413content',\n",
       " '002_dragon838552296986',\n",
       " '002_dragon844804994340content',\n",
       " '002_dragon852494520224',\n",
       " '002_dragon853396716332content',\n",
       " '002_dragon859035731542',\n",
       " '002_dragon866142856789',\n",
       " '002_dragon871663965498',\n",
       " '002_dragon872235195865content',\n",
       " '002_dragon875584409480content',\n",
       " '002_dragon882261761834',\n",
       " '002_dragon882895304044content',\n",
       " '002_dragon887388321534content',\n",
       " '002_dragon890168734255',\n",
       " '002_dragon890932882408content',\n",
       " '002_dragon898049280100content',\n",
       " '002_dragon908062144026content',\n",
       " '002_dragon912634163458content',\n",
       " '002_dragon917066359101',\n",
       " '002_dragon926841011669content',\n",
       " '002_dragon943687789736',\n",
       " '002_dragon953322195972content',\n",
       " '002_dragon955500430856content',\n",
       " '002_dragon956476021852content',\n",
       " '002_dragon959715935339content',\n",
       " '002_dragon966708873484content',\n",
       " '002_dragon966803059116content',\n",
       " '002_dragon975760983408content',\n",
       " '002_dragon975935048487content',\n",
       " '002_dragon980145547667content',\n",
       " '002_dragon981554492198content',\n",
       " '002_dragon984547041819',\n",
       " '002_dragon986057447477content',\n",
       " '002_dragon993740048225content',\n",
       " '002_dragon998760730906',\n",
       " '002_dragon998895130427content',\n",
       " '002a01c2724f',\n",
       " '002br',\n",
       " '002d01c26659',\n",
       " '002d16e0',\n",
       " '002d16e0content',\n",
       " '002e62',\n",
       " '002f40f4',\n",
       " '002hn',\n",
       " '002mk',\n",
       " '002time',\n",
       " '003',\n",
       " '0030',\n",
       " '003001bee908',\n",
       " '00301320383e01e0383fffc0148014005b13f8ea33c00030c7fca4ea31fcea37ff383e0fc0383807e0ea3003000013f0a214f8a21238127c12fea200fc13f0a2387007e0003013c0383c1f80380fff00ea03f815207d9f1c',\n",
       " '0031',\n",
       " '00316',\n",
       " '0031630926532',\n",
       " '0031847131601',\n",
       " '0032',\n",
       " '0033',\n",
       " '003301beebdc',\n",
       " '003366',\n",
       " '003399',\n",
       " '0034',\n",
       " '0034pulse_default',\n",
       " '0034tcnt',\n",
       " '0035',\n",
       " '003501beebdf',\n",
       " '00353241',\n",
       " '0036',\n",
       " '0037',\n",
       " '0038',\n",
       " '003801c67dab',\n",
       " '0039',\n",
       " '003901beebdf',\n",
       " '003_dragon056507828553_',\n",
       " '003_dragon340468286956_',\n",
       " '003_dragon346143145919_',\n",
       " '003_dragon402596564128_',\n",
       " '003_dragon426402787230_',\n",
       " '003_dragon744672031743_',\n",
       " '003_dragon838556541118_',\n",
       " '003br',\n",
       " '003fax',\n",
       " '003nl',\n",
       " '003pt',\n",
       " '004',\n",
       " '0040',\n",
       " '0040095e21234',\n",
       " '004093',\n",
       " '0040b0b1',\n",
       " '0040b530',\n",
       " '0041',\n",
       " '004201beebf9',\n",
       " '0043',\n",
       " '0043c77a',\n",
       " '0043fec0',\n",
       " '0044',\n",
       " '004499',\n",
       " '0044eb9c',\n",
       " '0044f6dc',\n",
       " '0045',\n",
       " '0045subroutine_servo_a5_init',\n",
       " '0046',\n",
       " '00465',\n",
       " '00466',\n",
       " '00467',\n",
       " '00468078',\n",
       " '0046937f',\n",
       " '00469d74',\n",
       " '00469d76',\n",
       " '0047',\n",
       " '0048',\n",
       " '004801c67e59',\n",
       " '0049',\n",
       " '0049servo_a5_pulse',\n",
       " '004nl',\n",
       " '005',\n",
       " '0050',\n",
       " '005004',\n",
       " '0050servo_a5_int',\n",
       " '005135',\n",
       " '0052',\n",
       " '0052servo_bit',\n",
       " '0053',\n",
       " '005301beedb7',\n",
       " '0054',\n",
       " '0055',\n",
       " '0056',\n",
       " '0057',\n",
       " '0058',\n",
       " '0059',\n",
       " '0059variable_servo_a5_pulse',\n",
       " '005d01bf6f13',\n",
       " '005hn',\n",
       " '005la',\n",
       " '005mk',\n",
       " '006',\n",
       " '0060',\n",
       " '0063',\n",
       " '00632d93',\n",
       " '0063825485257156_',\n",
       " '0063825485257156_content',\n",
       " '0063825785257156_',\n",
       " '0063825785257156_content',\n",
       " '00638260',\n",
       " '0063servo_enable',\n",
       " '0064',\n",
       " '006489',\n",
       " '0064setup_gap',\n",
       " '0065',\n",
       " '0066',\n",
       " '00664e75',\n",
       " '00667',\n",
       " '00667280',\n",
       " '0066993785257156_',\n",
       " '0066993785257156_content',\n",
       " '0066993a85257156_',\n",
       " '0066993a85257156_content',\n",
       " '00669945',\n",
       " '006765d8',\n",
       " '0067tctl1',\n",
       " '0068',\n",
       " '00681b6c',\n",
       " '0068a574',\n",
       " '0068bdbc',\n",
       " '0068uf',\n",
       " '0068uff',\n",
       " '0069',\n",
       " '00690340',\n",
       " '0069f5b8',\n",
       " '0069f5e0',\n",
       " '0069f628',\n",
       " '0069fa36',\n",
       " '006a0604',\n",
       " '006b57d8',\n",
       " '006c67cc',\n",
       " '006hn',\n",
       " '006hz',\n",
       " '006karnatakaindiaph',\n",
       " '006la',\n",
       " '006mk',\n",
       " '006nl',\n",
       " '006ov',\n",
       " '007',\n",
       " '0070',\n",
       " '0071',\n",
       " '0072',\n",
       " '0073',\n",
       " '00731864',\n",
       " '00734d74',\n",
       " '0074',\n",
       " '0075',\n",
       " '0076',\n",
       " '0076194298',\n",
       " '00762',\n",
       " '0077',\n",
       " '0077author',\n",
       " '0078',\n",
       " '0078email',\n",
       " '0079',\n",
       " '00794d20',\n",
       " '007a5580',\n",
       " '007b5860',\n",
       " '007br',\n",
       " '007f40',\n",
       " '007fb512f839780780780060141800401408a300c0140c00801404a400001400b3a3497e3801fffe1e227ea123',\n",
       " '007fb61280a2397e03f80f00781407007014030060140100e015c0a200c01400a400001500b3a248b512f0a222227ea127',\n",
       " '007fb8fca39039c00ff801d87e00ec003f007c82007882a200708200f01780a3481603a5c792c7fcb3aa017fb6fca331307daf38',\n",
       " '007fd',\n",
       " '007parkerbros',\n",
       " '007you',\n",
       " '008',\n",
       " '0080',\n",
       " '00800',\n",
       " '008000',\n",
       " '0081',\n",
       " '0082',\n",
       " '0082toc3',\n",
       " '0083',\n",
       " '0083903358',\n",
       " '0083903358caveats',\n",
       " '0084',\n",
       " '008482',\n",
       " '0085',\n",
       " '0085weekly',\n",
       " '0086',\n",
       " '0087',\n",
       " '008723514',\n",
       " '0087411652337',\n",
       " '0088',\n",
       " '0088a100',\n",
       " '0088cforc',\n",
       " '0088office',\n",
       " '0089',\n",
       " '0089535',\n",
       " '0089tmsk1',\n",
       " '008br',\n",
       " '008ov',\n",
       " '008scan1',\n",
       " '008scan1testa',\n",
       " '009',\n",
       " '0090',\n",
       " '00901a',\n",
       " '0091',\n",
       " '0092',\n",
       " '0093',\n",
       " '0093according',\n",
       " '0093bac',\n",
       " '0093calls',\n",
       " '0093contact',\n",
       " '0093no',\n",
       " '0093subroutine_initialize_module',\n",
       " '0095',\n",
       " '0095setup_pulse',\n",
       " '0096',\n",
       " '0097',\n",
       " '0097612',\n",
       " '0097616095',\n",
       " '0097616095a',\n",
       " '0098',\n",
       " '009801c29c90',\n",
       " '0098d3',\n",
       " '0099',\n",
       " '009901c29c95',\n",
       " '0099ff',\n",
       " '009a01c29c9a',\n",
       " '009be502',\n",
       " '009br',\n",
       " '009c01bee96b',\n",
       " '009ef6d0',\n",
       " '009f00b0',\n",
       " '009mk',\n",
       " '00a301bee7d1',\n",
       " '00am',\n",
       " '00assistant',\n",
       " '00b13250',\n",
       " '00c',\n",
       " '00c04fd7081f',\n",
       " '00c04fd97575',\n",
       " '00can',\n",
       " '00cf01bee7d2',\n",
       " '00dc',\n",
       " '00e03u',\n",
       " '00e2',\n",
       " '00e4',\n",
       " '00est',\n",
       " '00euro',\n",
       " '00ff',\n",
       " '00ffff',\n",
       " '00for',\n",
       " '00from',\n",
       " '00golden',\n",
       " '00h',\n",
       " '00i',\n",
       " '00ifcc',\n",
       " '00jan',\n",
       " '00joint',\n",
       " '00late',\n",
       " '00ms',\n",
       " '00ms0',\n",
       " '00msgraphic',\n",
       " '00n',\n",
       " '00noon',\n",
       " '00numbers',\n",
       " '00p',\n",
       " '00pm',\n",
       " '00pma',\n",
       " '00pmemu',\n",
       " '00pmregistration',\n",
       " '00pmtake',\n",
       " '00pmthe',\n",
       " '00pmweir',\n",
       " '00pmwhile',\n",
       " '00processor',\n",
       " '00r',\n",
       " '00readme',\n",
       " '00session',\n",
       " '00shipping',\n",
       " '00subject',\n",
       " '00subroutine_initialize_module',\n",
       " '00tim',\n",
       " '00time',\n",
       " '00total',\n",
       " '00u',\n",
       " '00ui',\n",
       " '00usd',\n",
       " '00use',\n",
       " '00v',\n",
       " '00variable_tcount',\n",
       " '00vga',\n",
       " '00what',\n",
       " '00which',\n",
       " '00x0c',\n",
       " '00x0f',\n",
       " '00you',\n",
       " '00z',\n",
       " '00以後',\n",
       " '01',\n",
       " '010',\n",
       " '0100',\n",
       " '01000000',\n",
       " '0100007f',\n",
       " '01002',\n",
       " '01002413',\n",
       " '0100a8c0',\n",
       " '0100after',\n",
       " '0100from',\n",
       " '0100hello',\n",
       " '0100hi',\n",
       " '0100jonathan',\n",
       " '0100kyle',\n",
       " '0100message',\n",
       " '0100mime',\n",
       " '0100ok',\n",
       " '0100on',\n",
       " '0100received',\n",
       " '0100reply',\n",
       " '0100subject',\n",
       " '0100this',\n",
       " '0100to',\n",
       " '0101',\n",
       " '0101e',\n",
       " '0102',\n",
       " '0103',\n",
       " '010307',\n",
       " '0104',\n",
       " '0106',\n",
       " '01062',\n",
       " '01063',\n",
       " '01063voice',\n",
       " '0106no',\n",
       " '0107',\n",
       " '010703',\n",
       " '010718',\n",
       " '0108',\n",
       " '010918',\n",
       " '0109toc3int',\n",
       " '010a',\n",
       " '010amicrosoft',\n",
       " '010kp',\n",
       " '010nl',\n",
       " '010ov',\n",
       " '011',\n",
       " '0110',\n",
       " '0111',\n",
       " '0112pactl',\n",
       " '0112servo_port',\n",
       " '01132',\n",
       " '0113tflg1',\n",
       " '0114',\n",
       " '01155',\n",
       " '0115518009',\n",
       " '011557est',\n",
       " '0115la',\n",
       " '0115servo_disable',\n",
       " '0115tel',\n",
       " '0116349',\n",
       " '0116349proposal',\n",
       " '0117',\n",
       " '01171875',\n",
       " '01184',\n",
       " '0119',\n",
       " '011a',\n",
       " '011aapple',\n",
       " '011commercial',\n",
       " '011fd',\n",
       " '011nl',\n",
       " '011ov',\n",
       " '012',\n",
       " '012002',\n",
       " '0122',\n",
       " '01223',\n",
       " '0123',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# equivalently: combine fit and transform into a single step using the fit_transform method\n",
    "X_train_dtm = vect.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23230x148930 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2289885 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the document-term matrix\n",
    "# This should be:\n",
    "# <23230x161925 sparse matrix of type '<class 'numpy.int64'>'\n",
    "# \twith 2305787 stored elements in Compressed Sparse Row format>\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7744x148930 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 745324 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data, too, into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Part 5: Building and evaluating a model\n",
    "\n",
    "We will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "# use the nb variable\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm and the fit() method\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm using the predict() function\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98579545454545459"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2941,    9],\n",
       "       [ 101, 4693]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9668    0.9969    0.9816      2950\n",
      "          1     0.9981    0.9789    0.9884      4794\n",
      "\n",
      "avg / total     0.9862    0.9858    0.9858      7744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_class, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.64139212e-28,   1.00000000e+00,   1.00000000e+00, ...,\n",
       "         6.59810021e-94,   1.38938632e-22,   1.00000000e+00])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predicted probabilities for X_test_dtm (poorly calibrated)\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99773816847330354"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Part 7: Examining a model for further insight\n",
    "\n",
    "We will examine the our **trained Naive Bayes model** to calculate the approximate **\"spamminess\" of each token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190382"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary of X using get_feature_names()\n",
    "# its length should be 161925\n",
    "X_train_tokens = CountVectorizer().fit(X).get_feature_names()\n",
    "len(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '000000',\n",
       " '0000000',\n",
       " '00000000',\n",
       " '000000000',\n",
       " '00000000000000',\n",
       " '000000000000000000000000000000049999999999999e9',\n",
       " '0000000000000000000000000000000500000000000000e9',\n",
       " '0000000000000016666l',\n",
       " '0000000000000017d',\n",
       " '00000000000000e',\n",
       " '000000000000received',\n",
       " '000000000001received',\n",
       " '0000000000status',\n",
       " '0000000001d0',\n",
       " '0000000001l0',\n",
       " '0000000010000000004l0',\n",
       " '0000000016',\n",
       " '000000001d0',\n",
       " '00000000message',\n",
       " '00000000x',\n",
       " '00000001',\n",
       " '00000001content',\n",
       " '00000001irdecode',\n",
       " '00000004',\n",
       " '00000010',\n",
       " '00000010pwm',\n",
       " '00000011',\n",
       " '0000001196',\n",
       " '00000049',\n",
       " '0000005',\n",
       " '0000006hz',\n",
       " '000000eb',\n",
       " '000001',\n",
       " '00000100',\n",
       " '00000100shaftencoder',\n",
       " '00000111',\n",
       " '000001bdaaa0',\n",
       " '000001bdb744',\n",
       " '000001bdc5a5',\n",
       " '000001bdcaf3',\n",
       " '000001bdd411',\n",
       " '000001bdd98c',\n",
       " '000001bdda70',\n",
       " '000001bde0a0',\n",
       " '000001bed6b7',\n",
       " '000001c20f35',\n",
       " '000001c642d0']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 50 tokens\n",
    "X_train_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['３ｄバーチャルｓｅｘメーカー',\n",
       " '４名紹介http',\n",
       " '４月度新規メンバー様応援企画パーティー開催予定',\n",
       " '４月２３日',\n",
       " '５月までのようなので興味のある方はお早めに',\n",
       " '５月中週末３人か４人ぐらいで',\n",
       " '５００円分のポイントが完全無料で自動追加されます',\n",
       " '６名のみとなります',\n",
       " 'ａ型',\n",
       " 'ａ美様現在未亡人でいらっしゃいます',\n",
       " 'ｂ９６',\n",
       " 'ｆカップ',\n",
       " 'ｇａｌ誌多数掲載',\n",
       " 'ｇｏｏｄ',\n",
       " 'ｇｒａｎｄｅｅ',\n",
       " 'ｇｒａｎｄｅｅの理念やシステムのご紹介',\n",
       " 'ｇｗこそ出会いのチャンスhttp',\n",
       " 'ｇｗです',\n",
       " 'ｇｗはこういう女の子と過ごしたいっす',\n",
       " 'ｇｗ特典あり',\n",
       " 'ｈなこと大好きな人ばかり',\n",
       " 'ｈな女の子が多いので',\n",
       " 'ｈな欲望や願望を胸に秘め',\n",
       " 'ｈにそんなに興味なかったのと少し怖いのもあるため',\n",
       " 'ｈのお相手しただけで',\n",
       " 'ｈゲームメーカーの決定版',\n",
       " 'ｈ度',\n",
       " 'ｈ目的の出会いも簡単です',\n",
       " 'ｈ８６',\n",
       " 'ｋ村',\n",
       " 'ｍ子様セーリングクルーザーをお持ちで',\n",
       " 'ｍ字開脚オナニーを机の下から盗撮',\n",
       " 'ｍａｉｌでのサポートは２４時間対応です',\n",
       " 'ｎ藤',\n",
       " 'ｏｌ',\n",
       " 'ｐｃ',\n",
       " 'ｐｃから簡単プロフィール作成',\n",
       " 'ｓクラス専門店',\n",
       " 'ｓ子様秘密が条件で',\n",
       " 'ｓｅｘを求めている',\n",
       " 'ｓｅｘを求めているのです',\n",
       " 'ｓｍ',\n",
       " 'ｔ165',\n",
       " 'ｔバックは',\n",
       " 'ｔバックはいていたらおならが左右に分散するのでなんか変な感じですけどね',\n",
       " 'ｔバックを購入しました',\n",
       " 'ｔ島',\n",
       " 'ｔ谷',\n",
       " 'ｗ６２',\n",
       " 'ｙ里様お互いがくつろげるような']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the last 50 tokens\n",
    "X_train_tokens[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.95200000e+03,   4.77000000e+02,   2.75000000e+02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  3.59800000e+03,   5.33300000e+03,   0.00000000e+00, ...,\n",
       "          2.00000000e+00,   3.00000000e+00,   2.00000000e+00]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of times each token appears in each class\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 148930)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows represent classes, columns represent tokens\n",
    "nb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1952.,   477.,   275., ...,     0.,     0.,     0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all HAM messages\n",
    "ham_token_count = nb.feature_count_[0, :]\n",
    "ham_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.59800000e+03,   5.33300000e+03,   0.00000000e+00, ...,\n",
       "         2.00000000e+00,   3.00000000e+00,   2.00000000e+00])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times each token appears across all SPAM messages\n",
    "spam_token_count = nb.feature_count_[1, :]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-28ba70e304ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a DataFrame of tokens with their separate ham and spam counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spam_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspam_token_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ham_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mham_token_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/beato/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2417\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/beato/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/beato/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2657\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/beato/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame(X_train_tokens, columns=['token'])\n",
    "tokens['spam_count'] = spam_token_count\n",
    "tokens['ham_count'] = ham_token_count\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# examine 5 random DataFrame rows using the sample() method\n",
    "tokens.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Before we can calculate the \"spamminess\" of each token, we need to avoid **dividing by zero** and account for the **class imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# add 1 to ham and spam counts to avoid dividing by 0\n",
    "tokens['spam_count'] = spam_token_count + 1\n",
    "tokens['ham_count'] = ham_token_count + 1\n",
    "\n",
    "tokens.sample(5, random_state=427)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# convert the ham and spam counts into percentage\n",
    "# by dividing them with nb.class_count_\n",
    "\n",
    "tokens['ham_count'] = tokens['ham_count'] / nb.class_count_[0]\n",
    "tokens['spam_count'] = tokens['spam_count'] / nb.class_count_[1]\n",
    "\n",
    "tokens.sample(5, random_state=427)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# calculate the ratio of spam-to-ham for each token\n",
    "tokens['spam_ratio'] = tokens['spam_count'] / tokens['ham_count']\n",
    "tokens.sample(5, random_state=427)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "tokens.sort_values('spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# look up the spam_ratio for a given token\n",
    "tokens[tokens['token'] == 'adobe'][['token','spam_ratio']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Naive bayes in a function for easier debugging and reading. \n",
    "\n",
    "### Final result: 99.69% avg f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Accuracy score 0.996900826446\n",
      "Confusion matrix [[2945    9]\n",
      " [  15 4775]]\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9949    0.9970    0.9959      2954\n",
      "          1     0.9981    0.9969    0.9975      4790\n",
      "\n",
      "avg / total     0.9969    0.9969    0.9969      7744\n",
      "\n",
      "Calculate predicted probabilities for X_test [  1.00000000e+000   1.00000000e+000   1.06405579e-190 ...,\n",
      "   1.00000000e+000   1.00000000e+000   1.00000000e+000]\n",
      "AUC 0.998754316358\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEFAULT_STOPS = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS\n",
    "PATH = 'data/spam_ham.csv'\n",
    "\n",
    "def do_naive_bayes(csv_path=PATH, stop_words=DEFAULT_STOPS, \n",
    "                   alpha=0.1, max_df=.9, min_df=0,\n",
    "                  ngram_range=(0, 4), random_state=None):\n",
    "    \"\"\"Takes\n",
    "    \n",
    "    \"\"\"\n",
    "    # read dataframe from csv\n",
    "    df = pd.read_csv(csv_path, header=0, names=['label', 'location', 'message'])\n",
    "    df = df.drop('location', axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # convert to numerical\n",
    "    df['label_num'] = df.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "    # assign to vars\n",
    "    X = df.message\n",
    "    y = df.label_num\n",
    "    \n",
    "    # vectorize dataset and transform into document-term matrix\n",
    "    vect = CountVectorizer(stop_words=stop_words, min_df=min_df, \n",
    "                           max_df=max_df, ngram_range=ngram_range)\n",
    "    X_dtm = vect.fit_transform(X)\n",
    "    \n",
    "    # split into test and train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dtm, y, random_state=random_state)\n",
    "    \n",
    "    # create multinomial naive bayes\n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    # fit to training data\n",
    "    nb.fit(X_train, y_train)\n",
    "    \n",
    "    # predict on test data\n",
    "    y_pred_class = nb.predict(X_test)\n",
    "    \n",
    "    # print metrics\n",
    "    print(\"Accuracy score\", metrics.accuracy_score(y_test, y_pred_class))\n",
    "    print(\"Confusion matrix\", metrics.confusion_matrix(y_test, y_pred_class))\n",
    "    print(\"Classification report\")\n",
    "    print(classification_report(y_test, y_pred_class, digits=4))\n",
    "    y_pred_prob = nb.predict_proba(X_test)[:, 1]\n",
    "    print(\"Calculate predicted probabilities for X_test\", y_pred_prob)\n",
    "    print(\"AUC\", metrics.roc_auc_score(y_test, y_pred_prob))\n",
    "\n",
    "\n",
    "\n",
    "# --- 0.1 won! 99.28\n",
    "# for x in np.arange(0.1,3.0,.3):\n",
    "#     print(\"USING\", x)\n",
    "#     do_naive_bayes(alpha=x)\n",
    "do_naive_bayes(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  0.4,  0.7,  1. ,  1.3,  1.6,  1.9,  2.2,  2.5,  2.8])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Part 9: Tuning the vectorizer (Challenge)\n",
    "\n",
    "Thus far, we have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n",
    "\n",
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "**Guidelines for tuning CountVectorizer:**\n",
    "\n",
    "- Use your knowledge of the **problem** and the **text**, and your understanding of the **tuning parameters**, to help you decide what parameters to tune and how to tune them.\n",
    "\n",
    "Tasks:\n",
    "1. **Experiment**, and let the data tell you the best approach!\n",
    "2. Try to reduce or increase the features and get a better score on the previous model. \n",
    "   * Score above a 99.5%? Tell us! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Part 10: Tuning the Laplacian Correction Factor (Challenge)\n",
    "\n",
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> class sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "> Parameters:\t\n",
    "alpha : float, optional (default=1.0)\n",
    "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "One of the parameters that we can tune in training a Multinomial Naive Bayes Classifier is the Laplacian Correction Factor.\n",
    "\n",
    "Tasks:\n",
    "1. Tweak the correction factor from 0-3 in increments of 0.1, 5, and 10, thus training multiple classifiers.\n",
    "2. Plot the precision-recall curves for these classifiers to compare and contrast."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
