{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOST Summer School"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python implementation of the Backpropagation algorithm is given below. This implementation is not optimized and therefore not very efficient. But it was coded in order to be easily understood by the reader.\n",
    "\n",
    "Use the Boolean OR, AND and XOR dataset as the input. \n",
    "\n",
    "Run the program by testing the three different boolean functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.200, error=2.447\n",
      ">epoch=1, lrate=0.200, error=2.335\n",
      ">epoch=2, lrate=0.200, error=2.227\n",
      ">epoch=3, lrate=0.200, error=2.127\n",
      ">epoch=4, lrate=0.200, error=2.038\n",
      ">epoch=5, lrate=0.200, error=1.960\n",
      ">epoch=6, lrate=0.200, error=1.895\n",
      ">epoch=7, lrate=0.200, error=1.840\n",
      ">epoch=8, lrate=0.200, error=1.795\n",
      ">epoch=9, lrate=0.200, error=1.758\n",
      "[[{'weights':  [0.12965772334007, 0.8328291113135146, 0.7992416732718532] },\n",
      "{'weights':  [0.21663295302524663, 0.4554170305160552, 0.3654547561932233] }]]\n",
      "[[{'weights':  [0.6316266616949571, 0.7709016231914724, 0.1081355435556114] },\n",
      "{'weights':  [-0.40168941745294573, 0.4581583685980768, -0.1758807206839592] }]]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "\tnetwork = list()\n",
    "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\tnetwork.append(hidden_layer)\n",
    "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\n",
    "# Taining backprop algorithm\n",
    "seed(1)\n",
    "\n",
    "# OR dataset\n",
    "#dataset = [[0,0,0],[0,1,1],[1,0,1],[1,1,0]]\n",
    "\n",
    "# AND dataset\n",
    "dataset = [[0,0,0],[0,1,0],[1,0,0],[1,1,1]]\n",
    "\n",
    "# XOR dataset\n",
    "dataset = [[0,0,0],[0,1,0],[1,0,0],[1,1,1]]\n",
    "\n",
    "\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.2, 10, n_outputs)\n",
    "\n",
    "for layer in network:\n",
    "    for ctr in range(0,(len(layer))):\n",
    "        if(ctr==0):\n",
    "            print(\"[[{'weights': \",layer[ctr]['weights'],\"},\")\n",
    "        elif(ctr < (len(layer)-1)):\n",
    "            print(\"{'weights': \",layer[ctr]['weights'],\"},\")\n",
    "        else:\n",
    "            print(\"{'weights': \",layer[ctr]['weights'],\"}]]\")\n",
    "\n",
    "\t#print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for testing. Just copy the output weights and assign it to network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-32-d15a6914321f>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-d15a6914321f>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    ghts': [-7.740341833598472, -1.2763941018119498, 3.60478086932667]}]]\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "\toutputs = forward_propagate(network, row)\n",
    "\treturn outputs.index(max(outputs))\n",
    "\n",
    "# Test making predictions with the network\n",
    "# AND dataset\n",
    "dataset = [[0,0,0],[0,1,0],[1,0,0],[1,1,1]]\n",
    "\n",
    "\n",
    "# Assign to network the output weights from the training program: \n",
    "network=[[{'weights':  [0.12965772334007, 0.8328291113135146, 0.7992416732718532] },\n",
    "{'weights':  [0.21663295302524663, 0.4554170305160552, 0.3654547561932233] }]]\n",
    "[[{'weights':  [0.6316266616949571, 0.7709016231914724, 0.1081355435556114] },\n",
    "{'weights':  [-0.40168941745294573, 0.4581583685980768, -0.1758807206839592] }]]\n",
    "ghts': [-7.740341833598472, -1.2763941018119498, 3.60478086932667]}]]\n",
    "  \n",
    "for row in dataset:\n",
    "\tprediction = predict(network, row)\n",
    "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Animals Dataset\n",
    "Now use the \"animals.csv\" as a dataset. Perform necessary preprocessing of the dataset. Divide the dataset for training and testing. You have to tweak the parameters of the algorithm such as the number of neurons in the hidden layer, learning rate and the number of epochs to run in order to get the accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: 100% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer size of (8, 14) gives an accuracy of 1.0!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "animals = pd.read_csv('animals.csv')\n",
    "y = animals['class']\n",
    "X = animals.drop('class', 1)\n",
    "X.drop('sample', 1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED)\n",
    "\n",
    "best_combo = 0\n",
    "best_accuracy = 0\n",
    "best_clf = None\n",
    "\n",
    "LAYER_SIZE_TESTS = []\n",
    "MAX = 20\n",
    "\n",
    "for x in range(1, MAX):\n",
    "    for y in range(1, MAX):\n",
    "        LAYER_SIZE_TESTS.append((y, x))\n",
    "\n",
    "for x, y in LAYER_SIZE_TESTS:\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(x, y), random_state=SEED)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_class = clf.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    \n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_combo = ((x, y), acc)\n",
    "        best_clf = clf\n",
    "        \n",
    "print('Hidden layer size of {0} gives an accuracy of {1}!'.format(*best_combo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_combo[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
